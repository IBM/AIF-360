{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook demonstrates the use of the adapted Adversarial Debiasing algorithm.\n",
    "\n",
    "The code has been updated to Tensorflow 2.X and can now be used to learn a fair classifier with respect to multiple sensitive attributes simultaneously.\n",
    "\n",
    "References:\n",
    "\n",
    "[Mitigating UnwantedBiases with Adversarial Learning](https://arxiv.org/pdf/1801.07593.pdf)  \n",
    "  \n",
    "B. H. Zhang, B. Lemoine, and M. Mitchell,    \n",
    "AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society, 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:\n",
      "`load_boston` has been removed from scikit-learn since version 1.2.\n",
      "\n",
      "The Boston housing prices dataset has an ethical problem: as\n",
      "investigated in [1], the authors of this dataset engineered a\n",
      "non-invertible variable \"B\" assuming that racial self-segregation had a\n",
      "positive impact on house prices [2]. Furthermore the goal of the\n",
      "research that led to the creation of this dataset was to study the\n",
      "impact of air quality but it did not give adequate demonstration of the\n",
      "validity of this assumption.\n",
      "\n",
      "The scikit-learn maintainers therefore strongly discourage the use of\n",
      "this dataset unless the purpose of the code is to study and educate\n",
      "about ethical issues in data science and machine learning.\n",
      "\n",
      "In this special case, you can fetch the dataset from the original\n",
      "source::\n",
      "\n",
      "    import pandas as pd\n",
      "    import numpy as np\n",
      "\n",
      "    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "    target = raw_df.values[1::2, 2]\n",
      "\n",
      "Alternative datasets include the California housing dataset and the\n",
      "Ames housing dataset. You can load the datasets as follows::\n",
      "\n",
      "    from sklearn.datasets import fetch_california_housing\n",
      "    housing = fetch_california_housing()\n",
      "\n",
      "for the California housing dataset and::\n",
      "\n",
      "    from sklearn.datasets import fetch_openml\n",
      "    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "for the Ames housing dataset.\n",
      "\n",
      "[1] M Carlisle.\n",
      "\"Racist data destruction?\"\n",
      "<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n",
      "\n",
      "[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n",
      "\"Hedonic housing prices and the demand for clean air.\"\n",
      "Journal of environmental economics and management 5.1 (1978): 81-102.\n",
      "<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
      ": LawSchoolGPADataset will be unavailable. To install, run:\n",
      "pip install 'aif360[LawSchoolGPA]'\n",
      "WARNING:root:No module named 'fairlearn': ExponentiatedGradientReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics import ClassificationMetric\n",
    "\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions import load_preproc_data_adult, load_preproc_data_compas, load_preproc_data_german\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from IPython.display import Markdown, display\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# import adapted AdversarialDebiasing code\n",
    "from aif360.algorithms.inprocessing.multi_attribute_adversarial_debiasing import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predicted_labels, true):\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(predicted_labels, true), dtype=tf.float32))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 . Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataset and split into train and test\n",
    "dataset_orig = load_preproc_data_compas()#load_preproc_data_adult()#\n",
    "\n",
    "privileged_groups = [{'sex': 1}, {'race': 1}]\n",
    "unprivileged_groups = [{'sex': 0}, {'race': 0}]\n",
    "\n",
    "dataset_orig_train, dataset_orig_test = dataset_orig.split([0.7], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Training Dataset shape"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3694, 10)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Favorable and unfavorable labels"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1.0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Protected attribute names"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sex', 'race']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Privileged and unprivileged protected attribute values"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1.]), array([1.])] [array([0.]), array([0.])]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Dataset feature names"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sex', 'race', 'age_cat=25 to 45', 'age_cat=Greater than 45', 'age_cat=Less than 25', 'priors_count=0', 'priors_count=1 to 3', 'priors_count=More than 3', 'c_charge_degree=F', 'c_charge_degree=M']\n"
     ]
    }
   ],
   "source": [
    "# print out some labels, names, etc.\n",
    "display(Markdown(\"#### Training Dataset shape\"))\n",
    "print(dataset_orig_train.features.shape)\n",
    "display(Markdown(\"#### Favorable and unfavorable labels\"))\n",
    "print(dataset_orig_train.favorable_label, dataset_orig_train.unfavorable_label)\n",
    "display(Markdown(\"#### Protected attribute names\"))\n",
    "print(dataset_orig_train.protected_attribute_names)\n",
    "display(Markdown(\"#### Privileged and unprivileged protected attribute values\"))\n",
    "print(dataset_orig_train.privileged_protected_attributes, \n",
    "      dataset_orig_train.unprivileged_protected_attributes)\n",
    "display(Markdown(\"#### Dataset feature names\"))\n",
    "print(dataset_orig_train.feature_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate metric for original training data\n",
    "\n",
    "Race and sex are the two sensitive attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitive attribute :  sex \n",
      "\n",
      "Train set: Difference in mean outcomes between unprivileged and privileged groups = -0.116761\n",
      "Test set: Difference in mean outcomes between unprivileged and privileged groups = -0.173738\n",
      "\n",
      "\n",
      "\n",
      "Sensitive attribute :  race \n",
      "\n",
      "Train set: Difference in mean outcomes between unprivileged and privileged groups = -0.133658\n",
      "Test set: Difference in mean outcomes between unprivileged and privileged groups = -0.129302\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for unprivileged_group, privileged_group in zip (unprivileged_groups, privileged_groups):\n",
    "    print('Sensitive attribute : ', list(unprivileged_group.keys())[0], '\\n')\n",
    "\n",
    "\n",
    "    # Metric for the original dataset\n",
    "    metric_orig_train = BinaryLabelDatasetMetric(dataset_orig_train, \n",
    "                                                unprivileged_groups=[unprivileged_group],\n",
    "                                                privileged_groups=[privileged_group])\n",
    "    \n",
    "    print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())\n",
    "    metric_orig_test = BinaryLabelDatasetMetric(dataset_orig_test, \n",
    "                                                unprivileged_groups=[unprivileged_group],\n",
    "                                                privileged_groups=[privileged_group])\n",
    "    print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_test.mean_difference())\n",
    "    print('\\n'*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = MinMaxScaler()#MaxAbsScaler()\n",
    "dataset_orig_train.features = min_max_scaler.fit_transform(dataset_orig_train.features)\n",
    "dataset_orig_test.features = min_max_scaler.transform(dataset_orig_test.features)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 . Debias multiple sensitive attributes \n",
    "\n",
    "3 performance metrics and 2 Fairness metrics to evaluate the implementation. \n",
    "> <b> Performance metrics:  </b>  \n",
    "> - Accuracy  \n",
    "> - Balanced Accuracy  \n",
    "> - AUC score  \n",
    "\n",
    "> <b> Fairness metrics:  </b>   \n",
    "> - Equal opportunity difference  \n",
    "> - Average odds Difference  \n",
    "\n",
    "##### NOTE:\n",
    "Current implementation supports an adversary that tries to enforce equality of Odds  \n",
    "since it gets both yHat and y during training (see [Mitigating UnwantedBiases with Adversarial Learning](https://arxiv.org/pdf/1801.07593.pdf)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sex', 'race']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example dataset\n",
    "features = dataset_orig_train.features\n",
    "labels = dataset_orig_train.labels\n",
    "protected_attributes = dataset_orig_train.protected_attributes\n",
    "\n",
    "dataset_orig_train.protected_attribute_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model\n",
    "\n",
    "The model consists of a classifier and an adversary network. At each training iteration, the classifier learns to predict a target label, and the adversary takes the classifier's predictions as well as  \n",
    "the true target label, and learns to predict multiple sensitive attribute values (e.g. race , sex) for that instance. Each model can be pretrained for several epochs before initiating adversarial learning.  \n",
    "The adversary can be provided with weights for each sensitive attribute, which is used in the loss computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training parameters\n",
    "nb_pretrain = 10\n",
    "batch_size = 64\n",
    "total_epochs = 40\n",
    "\n",
    "# weights for 'sex', 'race'\n",
    "# for Adult, use [3, 2]\n",
    "# for COMPAS, use [3, 5]\n",
    "\n",
    "loss_weights  = [3, 5 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining Classifier - Epoch 1, Loss: 0.5559677481651306\n",
      "Pretraining Classifier - Epoch 2, Loss: 0.6410130262374878\n",
      "Pretraining Classifier - Epoch 3, Loss: 0.5992349982261658\n",
      "Pretraining Classifier - Epoch 4, Loss: 0.5575165748596191\n",
      "Pretraining Classifier - Epoch 5, Loss: 0.5891270637512207\n",
      "Pretraining Classifier - Epoch 6, Loss: 0.5690146088600159\n",
      "Pretraining Classifier - Epoch 7, Loss: 0.6537361741065979\n",
      "Pretraining Classifier - Epoch 8, Loss: 0.5903810858726501\n",
      "Pretraining Classifier - Epoch 9, Loss: 0.7376126050949097\n",
      "Pretraining Classifier - Epoch 10, Loss: 0.723280131816864\n"
     ]
    }
   ],
   "source": [
    "# Create a trainer\n",
    "trainer = AdversarialDebiasor(loss_weights = loss_weights)\n",
    "\n",
    "# Pretrain the classifier model\n",
    "trainer.pretrain_classifier(features, labels, num_epochs=nb_pretrain, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate test set classification metrics after pre-training classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitive attribute :  sex\n",
      "\tEqual opportunity diff. : -0.07632051742723678\n",
      "\tAv. Odds diff.          : -0.097887890434921 \n",
      "\n",
      "\n",
      "Sensitive attribute :  race\n",
      "\tEqual opportunity diff. : -0.184298473062518\n",
      "\tAv. Odds diff.          : -0.2472622246144233 \n",
      "\n",
      "\n",
      "\n",
      "Accuracy        :  0.6546717\n",
      "Balanced accuracy :  0.6529976095872341\n",
      "AUC score         :  0.7046936833942261\n"
     ]
    }
   ],
   "source": [
    "# Evaluate metrics\n",
    "\n",
    "metrics = trainer.get_classification_metrics(dataset_orig_test)\n",
    "\n",
    "for key, classified_metric_debiasing_test in metrics.items():\n",
    "    print('Sensitive attribute : ', key)\n",
    "\n",
    "    eod = classified_metric_debiasing_test.equal_opportunity_difference()\n",
    "    aod = classified_metric_debiasing_test.average_odds_difference()\n",
    "    ti = classified_metric_debiasing_test.theil_index()\n",
    "    print('\\tEqual opportunity diff. :', eod)\n",
    "    print('\\tAv. Odds diff.          :', aod, '\\n'*2)\n",
    "\n",
    "\n",
    "predicted_labels = trainer.predict(dataset_orig_test.features)\n",
    "true_labels = dataset_orig_test.labels\n",
    "\n",
    "TPR = classified_metric_debiasing_test.true_positive_rate()\n",
    "TNR = classified_metric_debiasing_test.true_negative_rate()\n",
    "bal_acc_debiasing_test = 0.5*(TPR+TNR)\n",
    "\n",
    "# Should not change since only trained adversary\n",
    "print('\\nAccuracy        : ', accuracy(predicted_labels, true_labels).numpy())\n",
    "print('Balanced accuracy : ', bal_acc_debiasing_test)\n",
    "print('AUC score         : ', roc_auc_score(true_labels, trainer.predict_proba(dataset_orig_test.features)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretrain the adversary on trained classifier's outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining Adversary - Epoch 1, Loss: 5.051822662353516\n",
      "Pretraining Adversary - Epoch 2, Loss: 4.579408645629883\n",
      "Pretraining Adversary - Epoch 3, Loss: 4.724931716918945\n",
      "Pretraining Adversary - Epoch 4, Loss: 4.734898090362549\n",
      "Pretraining Adversary - Epoch 5, Loss: 4.242758274078369\n",
      "Pretraining Adversary - Epoch 6, Loss: 5.133546829223633\n",
      "Pretraining Adversary - Epoch 7, Loss: 4.510110855102539\n",
      "Pretraining Adversary - Epoch 8, Loss: 3.8390564918518066\n",
      "Pretraining Adversary - Epoch 9, Loss: 5.072484493255615\n",
      "Pretraining Adversary - Epoch 10, Loss: 4.4203290939331055\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer.pretrain_adversary(features, labels, protected_attributes, num_epochs=nb_pretrain, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train both models together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Classifier Loss: 0.8082079887390137, Adversary Loss: 5.030716896057129\n",
      "Epoch 2, Classifier Loss: 0.7027792930603027, Adversary Loss: 4.895318984985352\n",
      "Epoch 3, Classifier Loss: 0.7074283957481384, Adversary Loss: 4.274467468261719\n",
      "Epoch 4, Classifier Loss: 0.6176442503929138, Adversary Loss: 4.480430603027344\n",
      "Epoch 5, Classifier Loss: 0.6115831136703491, Adversary Loss: 4.599733352661133\n",
      "Epoch 6, Classifier Loss: 0.6636387705802917, Adversary Loss: 4.595342636108398\n",
      "Epoch 7, Classifier Loss: 0.6236449480056763, Adversary Loss: 4.6188178062438965\n",
      "Epoch 8, Classifier Loss: 0.5074901580810547, Adversary Loss: 4.653450965881348\n",
      "Epoch 9, Classifier Loss: 0.7306272387504578, Adversary Loss: 4.531330585479736\n",
      "Epoch 10, Classifier Loss: 0.5685167908668518, Adversary Loss: 4.804622650146484\n",
      "Epoch 11, Classifier Loss: 0.6385553479194641, Adversary Loss: 4.2814788818359375\n",
      "Epoch 12, Classifier Loss: 0.6575136780738831, Adversary Loss: 4.911703109741211\n",
      "Epoch 13, Classifier Loss: 0.5970854759216309, Adversary Loss: 4.847204208374023\n",
      "Epoch 14, Classifier Loss: 0.6553201675415039, Adversary Loss: 4.506861686706543\n",
      "Epoch 15, Classifier Loss: 0.6159197092056274, Adversary Loss: 4.692800521850586\n",
      "Epoch 16, Classifier Loss: 0.6266149282455444, Adversary Loss: 4.2257819175720215\n",
      "Epoch 17, Classifier Loss: 0.6853337287902832, Adversary Loss: 4.665722370147705\n",
      "Epoch 18, Classifier Loss: 0.6643732190132141, Adversary Loss: 5.010640621185303\n",
      "Epoch 19, Classifier Loss: 0.473066121339798, Adversary Loss: 5.06714391708374\n",
      "Epoch 20, Classifier Loss: 0.6643756031990051, Adversary Loss: 4.908823013305664\n",
      "Epoch 21, Classifier Loss: 0.723297119140625, Adversary Loss: 4.555747985839844\n",
      "Epoch 22, Classifier Loss: 0.6685534119606018, Adversary Loss: 4.404540538787842\n",
      "Epoch 23, Classifier Loss: 0.7187919616699219, Adversary Loss: 4.808209419250488\n",
      "Epoch 24, Classifier Loss: 0.6665875911712646, Adversary Loss: 5.335505485534668\n",
      "Epoch 25, Classifier Loss: 0.653114914894104, Adversary Loss: 4.270059108734131\n",
      "Epoch 26, Classifier Loss: 0.6431632041931152, Adversary Loss: 4.949394226074219\n",
      "Epoch 27, Classifier Loss: 0.5811151266098022, Adversary Loss: 4.995644569396973\n",
      "Epoch 28, Classifier Loss: 0.5230771899223328, Adversary Loss: 4.857267379760742\n",
      "Epoch 29, Classifier Loss: 0.5752798318862915, Adversary Loss: 4.6293559074401855\n",
      "Epoch 30, Classifier Loss: 0.6423143148422241, Adversary Loss: 4.800754547119141\n",
      "Epoch 31, Classifier Loss: 0.6921396851539612, Adversary Loss: 5.168484687805176\n",
      "Epoch 32, Classifier Loss: 0.6572132706642151, Adversary Loss: 4.453428745269775\n",
      "Epoch 33, Classifier Loss: 0.6915915012359619, Adversary Loss: 4.141293525695801\n",
      "Epoch 34, Classifier Loss: 0.5895373821258545, Adversary Loss: 4.766304016113281\n",
      "Epoch 35, Classifier Loss: 0.5869421362876892, Adversary Loss: 4.782857894897461\n",
      "Epoch 36, Classifier Loss: 0.6132718920707703, Adversary Loss: 4.341517448425293\n",
      "Epoch 37, Classifier Loss: 0.6813576221466064, Adversary Loss: 5.162693023681641\n",
      "Epoch 38, Classifier Loss: 0.5575804710388184, Adversary Loss: 4.530814170837402\n",
      "Epoch 39, Classifier Loss: 0.6398541331291199, Adversary Loss: 4.500638008117676\n",
      "Epoch 40, Classifier Loss: 0.5734829902648926, Adversary Loss: 3.973944664001465\n"
     ]
    }
   ],
   "source": [
    "# Train both models together\n",
    "trainer.train(features, labels, protected_attributes, num_epochs=total_epochs, batch_size=batch_size)\n",
    "\n",
    "predicted_labels = trainer.predict(dataset_orig_test.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate test set classification metrics after debiasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitive attribute :  sex \n",
      "\n",
      "\tEqual opportunity diff. : 0.05910887531440889\n",
      "\tAv. Odds diff.          : 0.031224620498273375 \n",
      "\n",
      "\n",
      "Sensitive attribute :  race \n",
      "\n",
      "\tEqual opportunity diff. : -0.06104868913857686\n",
      "\tAv. Odds diff.          : -0.1180623770236495 \n",
      "\n",
      "\n",
      "Accuracy          :  0.6483586\n",
      "Balanced accuracy :  0.6434343595852354\n",
      "AUC score         :  0.6887330812340606\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.get_classification_metrics(dataset_orig_test)\n",
    "\n",
    "for key, classified_metric_debiasing_test in metrics.items():\n",
    "    print('Sensitive attribute : ', key, '\\n')\n",
    "\n",
    "    TPR = classified_metric_debiasing_test.true_positive_rate()\n",
    "    TNR = classified_metric_debiasing_test.true_negative_rate()\n",
    "    bal_acc_debiasing_test = 0.5*(TPR+TNR)\n",
    "    eod = classified_metric_debiasing_test.equal_opportunity_difference()\n",
    "    aod = classified_metric_debiasing_test.average_odds_difference()\n",
    "    ti = classified_metric_debiasing_test.theil_index()\n",
    "    print('\\tEqual opportunity diff. :', eod)\n",
    "    print('\\tAv. Odds diff.          :', aod, '\\n'*2)\n",
    "TPR = classified_metric_debiasing_test.true_positive_rate()\n",
    "TNR = classified_metric_debiasing_test.true_negative_rate()\n",
    "bal_acc_debiasing_test = 0.5*(TPR+TNR)\n",
    "\n",
    "\n",
    "\n",
    "print('Accuracy          : ', accuracy(predicted_labels, true_labels).numpy())\n",
    "print('Balanced accuracy : ', bal_acc_debiasing_test)\n",
    "print('AUC score         : ', roc_auc_score(true_labels, trainer.predict_proba(dataset_orig_test.features)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitive attribute :  sex\n",
      "Test set: Difference in mean outcomes between unprivileged and privileged groups = -0.012273\n",
      "\n",
      "\n",
      "Sensitive attribute :  race\n",
      "Test set: Difference in mean outcomes between unprivileged and privileged groups = -0.147379\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dmetrics = trainer.get_dataset_metrics(dataset_orig_test)\n",
    "\n",
    "for key, classified_metric_debiasing_test in metrics.items():\n",
    "    print('Sensitive attribute : ', key)\n",
    "    print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % classified_metric_debiasing_test.mean_difference())\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "design",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
